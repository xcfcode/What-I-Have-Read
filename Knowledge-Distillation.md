# Knowledge Distillation

[Knowledge Distillation Slides](slides/presentation/Knowledge%20Distillation.pdf)

| Paper | Conference |
| :---: | :---: |
|[Noisy Self-Knowledge Distillation for Text Summarization](https://arxiv.org/abs/2009.07032)||
|[Distilling Knowledge Learned in BERT for Text Generation](https://arxiv.org/abs/1911.03829)|ACL20|
|[Zero-Shot Cross-Lingual Abstractive Sentence Summarization through Teaching Generation and Attention](https://www.aclweb.org/anthology/P19-1305/)|ACL19|
| [Distilling Task-Specific Knowledge from BERT into Simple Neural Networks](https://arxiv.org/abs/1903.12136) | |
| [Multilingual Neural Machine Translation with Knowledge Distillation](https://arxiv.org/abs/1902.10461)| ICLR19 |
| [BAM! Born-Again Multi-Task Networks for Natural Language Understanding](https://arxiv.org/abs/1907.04829) |ACL19 |
|[Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding](https://arxiv.org/abs/1904.09482)||
| [Exploiting the Ground-Truth: An Adversarial Imitation Based Knowledge Distillation Approach for Event Detection](https://www.aaai.org/ojs/index.php/AAAI/article/view/4649) | AAAI19 |
| [Distilling Knowledge for Search-based Structured Prediction](https://www.aclweb.org/anthology/P18-1129/) | ACL18 |
| On-Device Neural Language Model based Word Prediction | COLING18 |
| [Zero-Shot Cross-Lingual Neural Headline Generation](https://dl.acm.org/doi/10.1109/TASLP.2018.2842432) | IEEE/ACM TRANSACTIONS 18 |
| [Cross-lingual Distillation for Text Classification](https://arxiv.org/abs/1705.02073) | ACL17 |
| DOMAIN ADAPTATION OF DNN ACOUSTIC MODELS USING KNOWLEDGE DISTILLATION | ICASSP17 |
| [Sequence-Level Knowledge Distillation](https://arxiv.org/abs/1606.07947) | EMNLP16 |
| Distilling Word Embeddings: An Encoding Approach | CIKM16 |
| [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) | NIPS14 Deep Learning Workshop|


