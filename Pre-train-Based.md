# Pre-train Based

[Advanced pre-training language models a brief introduction Slides](slides/presentation/Advanced%20pre-training%20language%20models%20a%20brief%20introduction.pdf)

1. **Syntax-Enhanced Pre-trained Model** *Zenan Xu, Daya Guo, Duyu Tang, Qinliang Su, Linjun Shou, Ming Gong, Wanjun Zhong, Xiaojun Quan, Nan Duan, Daxin Jiang* [[pdf]](https://arxiv.org/abs/2012.14116)
2. **Language Models are Open Knowledge Graphs** *Chenguang Wang, Xiao Liu, Dawn Song* [[pdf]](https://arxiv.org/abs/2010.11967)

## Survey

| Paper | Conference |
| :---: | :---: |
|[Pre-trained Models for Natural Language Processing: A Survey](https://arxiv.org/abs/2003.08271)||

## Knowledge
| Paper | Conference |
| :---: | :---: |
|K-ADAPTER: Infusing Knowledge into Pre-Trained Models with Adapters||
|Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model |ICLR20|
|A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation|TACL19|
|ERNIE: Enhanced Representation through Knowledge Integration||
|ERNIE: Enhanced Language Representation with Informative Entities|ACL19|

## Multi-Lingual
| Paper | Conference |
| :---: | :---: |
|[Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210)||

## Generation
| Paper | Conference |
| :---: | :---: |
|[QURIOUS: Question Generation Pretraining for Text Generation](https://arxiv.org/pdf/2004.11026.pdf)|ACL20|
|BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension||
|Unified Language Model Pre-training for Natural Language Understanding and Generation||
|MASS: Masked Sequence to Sequence Pre-training for Language Generation|ICML19|

| Paper | Conference |
| :---: | :---: |
|[Cross-Thought for Sentence Encoder Pre-training](https://arxiv.org/abs/2010.03652)|EMNLP2020|
|[Big Bird: Transformers for Longer Sequences](https://arxiv.org/abs/2007.14062)||
|[Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)||
|[Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks](https://arxiv.org/abs/2004.10964)|ACL20|
|Make Lead Bias in Your Favor : A Simple and Effective Method for News Summarization||
|DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation||
| What Does BERT Look At? An Analysis of BERT’s Attention | EMNLP19 |
|SpanBERT: Improving Pre-training by Representing and Predicting Spans||
|XLNet: Generalized Autoregressive Pretraining for Language Understanding||
|Pre-Training with Whole Word Masking for Chinese BERT||
| BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|NAACL19|
|Linguistic Knowledge and Transferability of Contextual Representations|NAACL19|
|Improving Language Understanding by Generative Pre-Training||
|Deep contextualized word representations|NAACL18|

