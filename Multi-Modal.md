# Multi Modal

| Paper | Conference |
| :---: | :---: |
|[Unicoder-VL: A Universal Encoder for Vision and Language by Cross-modal Pre-training](https://arxiv.org/abs/1908.06066)|AAAI20|
|[Storytelling from an Image Stream Using Scene Graphs](http://www.sdspeople.fudan.edu.cn/zywei/paper/2020/wang-aaai-2020.pdf)|AAAI20|
|[Multimodal Transformer for Unaligned Multimodal Language Sequences](https://arxiv.org/abs/1906.00295)||
|[M-BERT: Injecting Multimodal Information in the BERT Structure](https://arxiv.org/pdf/1908.05787.pdf)||
|[OmniNet: A unified architecture for multi-modal multi-task learning](https://arxiv.org/pdf/1907.07804.pdf)||
|[Multi-Modal Language Analysis with Hierarchical Interaction-Level and Selection-Level Attentions](https://ieeexplore.ieee.org/document/8785040)|ICME19||
|[VATEX:A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research](https://arxiv.org/abs/1904.03493)|ICCV19|
|[Multimodal Abstractive Summarization for How2 Videos](https://www.aclweb.org/anthology/P19-1659/)|ACL19|
|[Words Can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors](https://arxiv.org/abs/1811.09362)|AAAI19|
|[How2:A Large-scale Dataset for Multimodal Language Understanding](https://arxiv.org/abs/1811.00347)|NIPS18|
|[Attention Strategies for Multi-Source Sequence-to-Sequence Learning](https://arxiv.org/abs/1704.06567)|ACL17|


