# Prompt Learning

1. **Input-Tuning: Adapting Unfamiliar Inputs to Frozen Pretrained Models** *Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen, Nanning Zheng, Jian-Guang Lou* [[pdf]](https://arxiv.org/abs/2203.03131)
2. **PromDA: Prompt-based Data Augmentation for Low-Resource NLU Tasks** *Yufei Wang, Can Xu, Qingfeng Sun, Huang Hu, Chongyang Tao, Xiubo Geng, Daxin Jiang* `ACL 2022` [[pdf]](https://arxiv.org/abs/2202.12499)
3. **Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?** *Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer* [[pdf]](https://arxiv.org/abs/2202.12837) [[code]](https://github.com/Alrope123/rethinking-demonstrations)
4. **Zero-shot Cross-lingual Transfer of Prompt-based Tuning with a Unified Multilingual Prompt** *Lianzhe Huang, Shuming Ma, Dongdong Zhang, Furu Wei, Houfeng Wang* [[pdf]](https://arxiv.org/abs/2202.11451)
5. **Discourse-Aware Prompt Design for Text Generation** *Marjan Ghazvininejad, Vladimir Karpukhin, Asli Celikyilmaz* [[pdf]](https://arxiv.org/abs/2112.05717)
6. **Enhancing Cross-lingual Prompting with Mask Token Augmentation** *Meng Zhou, Xin Li, Yue Jiang, Lidong Bing* [[pdf]](https://arxiv.org/abs/2202.07255)
7. **Discrete and Soft Prompting for Multilingual Models** *Mengjie Zhao, Hinrich Schütze* [[pdf]](https://aclanthology.org/2021.emnlp-main.672/) [[code]](https://github.com/mprompting/xlmrprompt)
8. **AdaPrompt: Adaptive Model Training for Prompt-based NLP** *Yulong Chen, Yang Liu, Li Dong, Shuohang Wang, Chenguang Zhu, Michael Zeng, Yue Zhang* [[pdf]](https://arxiv.org/abs/2202.04824)
9. **Controllable Generation from Pre-trained Language Models via Inverse Prompting** *Xu Zou, Da Yin, Qingyang Zhong, Ming Ding, Zhilin Yang, Jie Tang* `KDD 2021` [[pdf]](https://arxiv.org/abs/2103.10685) [[demo]](https://pretrain.aminer.cn/apps/poetry.html) [[code]](https://github.com/THUDM/iPrompt) [[blog]](https://mp.weixin.qq.com/s/3BZkTw-2AIsLQGbeIvI-yg)
10. **Black-box Prompt Learning for Pre-trained Language Models** *Shizhe Diao, Xuechun Li, Yong Lin, Zhichao Huang, Tong Zhang* [[pdf]](https://arxiv.org/abs/2201.08531)
11. **Context-Tuning: Learning Contextualized Prompts for Natural Language Generation** *Tianyi Tang, Junyi Li, Wayne Xin Zhao* [[pdf]](https://arxiv.org/abs/2201.08670)
12. **Discrete and Soft Prompting for Multilingual Models** *Mengjie Zhao, Hinrich Schütze* `EMNLP 2021` [[pdf]](https://aclanthology.org/2021.emnlp-main.672/) [[code]](https://github.com/mprompting/xlmrprompt)
13. **GPT Understands, Too** *Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang* `` [[pdf]](https://arxiv.org/abs/2103.10385) [[code]](https://github.com/THUDM/P-tuning) [[code]](https://github.com/bojone/P-tuning)
3. **OpenPrompt: An Open-source Framework for Prompt-learning** *Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun* [[pdf]](https://arxiv.org/abs/2111.01998)

